{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "2015-12-22 \n",
    "\n",
    "- tanh units\n",
    "    - (dropout generally 0.5)\n",
    "    - dense hingeloss ok\n",
    "    - identity // sigmoid hingeloss doesn't seem to work (inner scale 1.05)\n",
    "    - softmax_binary_crossentropy doesnt' seem to work.\n",
    "- relu units\n",
    "    - inner scale 1.0\n",
    "\n",
    "#####\n",
    " TODO 7/2015 :: Figure out why the .ipynb has to be in ~/keras \n",
    " (and NOT ~/notebooks, and NOT ~/keras/keras, and NOT ~/keras/keras/notebooks)\n",
    " in order to run from keras.optimizers import HF\n",
    " UPDATE 12/2015 :: I think I over-wrote this code so it doesn't matter any more.\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import HF\n",
    "\n",
    "\n",
    "#### PARAMETERS #########\n",
    " Hyperparameters to search\n",
    " - RNN hidden intialization.  Something around 1-1.1 is reasonable.\n",
    "     -> implemented as rotation(identity * [1..1.1]), but vary rotation\n",
    "     -> implemented\n",
    "\n",
    " - RNN input initialization.  Gaussian(0.1) ish?\n",
    "     - input as init = normal(shape, scale=0.05)\n",
    "\n",
    " - Centering using mean-subtraction.  This is actually a signal, so be careful.\n",
    "     - X_train = X_train - mean()\n",
    "    for row in range(a.shape[0]):\n",
    "        a[row,:,:] = a[row,:,:] - np.mean( a[row,:,:] )\n",
    "    --> implemented ++\n",
    "\n",
    " TODO\n",
    " - validation holdout set.\n",
    " - softmax gives very bad values (-16 forever?)\n",
    "\n",
    "# TO Try\n",
    "- centering / not centering\n",
    "- slightly greater than 1\n",
    "- rotated identity\n",
    "- start with easy trials, get harder later.\n",
    "\n",
    "\n",
    "## RESULTS\n",
    "\n",
    "\n",
    "#### tanh / dense hingeloss / identity / rmsprop / dropout 0.75 --> ok model\n",
    "dense_input_dim      512\n",
    "dense_output_dim     1\n",
    "do_center_X          0\n",
    "dropout              0.75\n",
    "final_layer_type     'dense_hingeloss'\n",
    "inner_activation     'tanh'\n",
    "optimizer            'RMSprop'\n",
    "regenerate_datasets  1\n",
    "rmsprop_clipvalue    None\n",
    "rmsprop_lr           0.001\n",
    "rnn_1_hidden_dim     512\n",
    "rnn_1_input_dim      2\n",
    "rnn_init_scale       1 --> probably way too large\n",
    "rnn_inner_init_type  'identity'\n",
    "rnn_inner_scale      1.0\n",
    "100000/100000 [==============================] - 281s - loss: 0.4130 - val_loss: 0.2956\n",
    "Epoch 00006: val_loss improved from 0.46421 to 0.29556, saving model to /home/avaughan/Dropbox/rnn/checkpoint_2016-01-12_01-37-11.hdf5\n",
    "\n",
    "##### tanh / dense hingeloss / identity / rmsprop / dropout 0.75 --> STUCK\n",
    "-------------------  -----------------------------\n",
    "dense_input_dim      512\n",
    "dense_output_dim     1\n",
    "do_center_X          1\n",
    "dropout              0\n",
    "final_layer_type     'softmax_binary_crossentropy'\n",
    "inner_activation     'tanh'\n",
    "optimizer            'RMSprop'\n",
    "regenerate_datasets  1\n",
    "rmsprop_clipvalue    None\n",
    "rmsprop_lr           1e-06\n",
    "rnn_1_hidden_dim     512\n",
    "rnn_1_input_dim      2\n",
    "rnn_init_scale       0.001\n",
    "rnn_inner_init_type  'identity'\n",
    "rnn_inner_scale      1.0\n",
    "-------------------  -----------------------------\n",
    "\n",
    "\n",
    "#### tanh / sigmoid hingeloss / identity / rmsprop / dropout 0 or 0.5 --> STUCK\n",
    "dense_input_dim      512\n",
    "dense_output_dim     1\n",
    "do_center_X          1\n",
    "dropout              0\n",
    "final_layer_type     'sigmoid_hingeloss'\n",
    "inner_activation     'tanh'\n",
    "optimizer            'RMSprop'\n",
    "regenerate_datasets  1\n",
    "rmsprop_clipvalue    None\n",
    "rmsprop_lr           1e-06\n",
    "rnn_1_hidden_dim     512\n",
    "rnn_1_input_dim      2\n",
    "rnn_init_scale       0.001\n",
    "rnn_inner_init_type  'identity'\n",
    "rnn_inner_scale      1.0\n",
    "-------------------  -------------------\n",
    "Epoch 12/300\n",
    "100000/100000 [==============================] - 333s - loss: 0.5014 - val_loss: 0.4912\n",
    "\n",
    "### RELU / dense hingeloss / identity / rmsprop 1E-6 / dropout  0.5 --> WORKS but eventually throws a nan.  Later though!\n",
    "Note, still have 2d sigmoid layer here.\n",
    "Changing to sigmoid hingeloss throws a nan sooner - still saturating?\n",
    "-------------------  -------------------\n",
    "dense_input_dim      128\n",
    "dense_output_dim     1\n",
    "do_center_X          1\n",
    "dropout              0.5\n",
    "final_layer_type     'dense_hingeloss'\n",
    "inner_activation     'relu'\n",
    "optimizer            'RMSprop'\n",
    "regenerate_datasets  0\n",
    "rmsprop_clipvalue    None\n",
    "rmsprop_lr           1E-6\n",
    "rnn_1_hidden_dim     128\n",
    "rnn_1_input_dim      2\n",
    "rnn_init_scale       0.001\n",
    "rnn_inner_init_type  'identity'\n",
    "rnn_inner_scale      1.0\n",
    "-------------------  -------------------\n",
    "\n",
    "rnn_output_scale     0.0001 --> NAN on batch 119.\n",
    "\n",
    "##\n",
    "final_layer_type     'sigmoid_hingeloss' --> Batchwise loss was NaN in batch 2302!\n",
    "##\n",
    "rnn_output_scale     0.00001 --> NAN loss on batch 119/2000.  Not sure.  Keep bashing on this, it's close.\n",
    "\n",
    "## RUNS\n",
    "but is basically stuck... long asymptote at 0.4958 loss\n",
    "-------------------  -------------------\n",
    "dense_input_dim      128\n",
    "dense_output_dim     1\n",
    "do_center_X          1\n",
    "dropout              0.5\n",
    "final_layer_type     'sigmoid_hingeloss'\n",
    "inner_activation     'relu'\n",
    "optimizer            'RMSprop'\n",
    "regenerate_datasets  0\n",
    "rmsprop_clipvalue    None\n",
    "rmsprop_lr           0.001\n",
    "rnn_1_hidden_dim     128\n",
    "rnn_1_input_dim      2\n",
    "rnn_init_scale       0.001\n",
    "rnn_inner_init_type  'identity'\n",
    "rnn_inner_scale      1.0\n",
    "rnn_output_scale     1e-05\n",
    "-------------------  -------------------\n",
    "This also has ... model.add(Dense( 2, init = lambda shape: normal(shape,scale=params.rnn_output_scale), activation='sigmoid'))\n",
    "as the penultimate layer\n",
    "--> doesn't work with dropout 0.9 --> NAN\n",
    "--> 0.75 works with 128 batch size.\n",
    "--> Tried SGD with learning rate 1e-3..1e-5 --> failes with nan.\n",
    "--> Back to RMSprop, with inner scale up to 1.1.  Stops at 0.495 error again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Using Theano backend.\n",
      "Backend is ::  theano\n",
      "Done at  2016-01-12 23:29:54.704273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "########## IMPORTS AND UTILITY FUNCITONS / CLASSES ###########\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import theano as T\n",
    "import pandas\n",
    "import time, random, sys\n",
    "\n",
    "from keras import backend\n",
    "print('Backend is :: ',backend._BACKEND)\n",
    "\n",
    "#from keras.datasets.data_utils import get_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.cm as cm\n",
    "\n",
    "from __future__ import print_function\n",
    "from six.moves import range\n",
    "from datetime import datetime\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "# My function for generating poisson datasets\n",
    "import generate_poisson_dataset\n",
    "reload(generate_poisson_dataset)\n",
    "\n",
    "# Utility class stolen from 3.3\n",
    "class SimpleNameSpace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "    def __repr__(self):\n",
    "        keys = sorted(self.__dict__)\n",
    "        items = (\"{}={!r}\".format(k, self.__dict__[k]) for k in keys)\n",
    "        return \"{}({})\".format(type(self).__name__, \", \".join(items))\n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "    \n",
    "print('Done at ',datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO 7/2015 :: Figure out why the .ipynb has to be in ~/keras \n",
    "# (and NOT ~/notebooks, and NOT ~/keras/keras, and NOT ~/keras/keras/notebooks)\n",
    "# in order to run from keras.optimizers import HF\n",
    "# UPDATE 12/2015 :: I think I over-wrote this code so it doesn't matter any more.\n",
    "#from keras.optimizers import SGD\n",
    "#from keras.optimizers import HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with cell at 2016-01-19 20:06:38.417384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### Function to define model #########\n",
    "\n",
    "def define_and_compile_model(params):\n",
    "    \n",
    "    # Print parameters\n",
    "    params_str = str(params)\n",
    "    print( 'Compiling with parameters as follows:')\n",
    "    print( tabulate(string.split(p,'=') for p in string.split(params_str[9:-1],',')))\n",
    "    \n",
    "   \n",
    "    ####### Initialize inner weights of RNN #####\n",
    "\n",
    "    def normal_radius(shape,scale=1):\n",
    "        normal_init = np.random.randn(*shape)\n",
    "        radius_init = max(abs(sp.linalg.eigvals(normal_init)))\n",
    "        return K.variable( normal_init / radius_init * scale )\n",
    "\n",
    "    if params.rnn_inner_init_type == 'identity':\n",
    "        rnn_inner_init = identity\n",
    "    elif params.rnn_inner_init_type == 'normal_radius':\n",
    "        rnn_inner_init = normal_radius\n",
    "    elif params.rnn_inner_init_type == 'orthogonal':\n",
    "        rnn_inner_init = orthogonal\n",
    "    else:\n",
    "        raise Exception('params.rnn_inner_init_type not found')\n",
    "\n",
    "    ####### Initialize Final Layer #######\n",
    "\n",
    "    if params.final_layer_type == 'dense_hingeloss':\n",
    "        final_layer = Dense(1, init=lambda shape: normal(shape,scale=0.1))\n",
    "        loss_fn = 'hinge'\n",
    "        optimizer_choice_values = (-1,1)\n",
    "    \n",
    "    elif params.final_layer_type == 'softmax_binary_crossentropy':\n",
    "        #final_layer = Dense(1,activation='softmax',init=lambda shape: normal(shape,scale=0.1))\n",
    "        final_layer = Dense(1,activation='softmax',init='glorot_uniform')\n",
    "        loss_fn = binary_crossentropy\n",
    "        optimizer_choice_values = (0,1)\n",
    "        \n",
    "    elif params.final_layer_type == 'softmax_categorical_crossentropy':\n",
    "        final_layer = Dense(2,activation='softmax')\n",
    "        loss_fn = categorical_crossentropy\n",
    "        optimizer_choice_values = (0,1)\n",
    "    \n",
    "    elif params.final_layer_type == 'sigmoid_hingeloss':\n",
    "        final_layer = Dense(1,activation='sigmoid', init=lambda shape: normal(shape,scale=0.1))\n",
    "        loss_fn = 'hinge'\n",
    "        optimizer_choice_values = (0,1)\n",
    "    \n",
    "    else:\n",
    "        raise Exception('params.final_layer_type not found :: ',params.final_layer_type)\n",
    "        \n",
    "\n",
    "    ####### Define loss and optimizer for model ######\n",
    "\n",
    "    def two_afc_loss(y_true, y_pred):\n",
    "        # Possible contributions to loss\n",
    "        mean_pre_output  = T.mean(abs(y_pred[:-1]))\n",
    "        not_rewarded = 1 - ( T.gt( y_true[-1] * y_pred[-1] , abs(y_true[-1]) ) ) # change 1 --> abs(y_true[-1])\n",
    "        binary_choice_error = 1 - T.gt(abs(y_pred[-1]-y_true[-1]),0) # 0 if correct, 1 if incorrect\n",
    "        abs_choice_error = abs(y_true[-1] - y_pred[-1])           # abs magnitude of error\n",
    "        # Output loss :: we include a T.zeros_like to make sure that our objective is appropriately broadcast to the size of the original inputs.\n",
    "        return abs_choice_error + T.zeros_like(y_true) # convert to a loss function, where 1 is a failed trial\n",
    "\n",
    "    if params.optimizer == 'SGD':\n",
    "        #optimizer = optimizers.SGD(lr=1e-4, decay=1e-5, momentum=0.99, nesterov=True)\n",
    "        if params.sgd_clipvalue is None:\n",
    "            optimizer = optimizers.SGD(lr=params.sgd_lr, decay=params.sgd_decay, momentum=params.sgd_momentum, nesterov=params.sgd_nesterov)\n",
    "        else:\n",
    "            optimizer = optimizers.SGD(lr=params.sgd_lr, decay=params.sgd_decay, momentum=params.sgd_momentum, nesterov=params.sgd_nesterov,clipvalue=params.sgd_clipvalue)\n",
    "    elif params.optimizer == 'RMSprop':\n",
    "        # Leave at default I guess?\n",
    "        if params.rmsprop_clipvalue is None:\n",
    "            optimizer = optimizers.RMSprop()\n",
    "        else:\n",
    "            optimizer = optimizers.RMSprop(clipvalue=params.rmsprop_clipvalue)\n",
    "            \n",
    "    else:\n",
    "        raise Exception('params.optimizer not found :: ', params.optimizer)\n",
    "\n",
    "    ####### Define model ######\n",
    "\n",
    "    print('Building  model...')\n",
    "    model = Sequential()\n",
    "    model.add( recurrent.SimpleRNN( params.rnn_1_hidden_dim,\n",
    "                                   input_shape      = (1001,2),\n",
    "                                   activation       = params.inner_activation,\n",
    "                                   return_sequences = False,\n",
    "                                   init             = lambda shape: normal(shape,scale=params.rnn_init_scale),\n",
    "                                   inner_init       = lambda shape: rnn_inner_init(shape,scale=params.rnn_inner_scale),\n",
    "                                   ))\n",
    "    model.add(Dropout(params.dropout))\n",
    "    model.add(Dense( 2,\n",
    "                    init = lambda shape: normal(shape,scale=params.rnn_output_scale),\n",
    "                    activation='sigmoid'))\n",
    "    model.add( final_layer )\n",
    "\n",
    "    ######## Compile ########\n",
    "    \n",
    "    print('Compiling')\n",
    "    t_start = time.time()\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    print('...done compiling in %.1f seconds' %(time.time()-t_start),datetime.now())\n",
    "    return model\n",
    "\n",
    "print('\\nDone with cell at',datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with cell at 2016-01-19 19:02:37.962221\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "######### SPECIFY PARAMETERS ##############\n",
    "###########################################\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import recurrent\n",
    "import keras.optimizers as optimizers\n",
    "import keras.callbacks as callbacks\n",
    "from keras.objectives import hinge, binary_crossentropy, categorical_crossentropy\n",
    "from keras.initializations import normal,identity,orthogonal\n",
    "from keras import backend as K\n",
    "reload(callbacks)\n",
    "reload(generate_poisson_dataset)\n",
    "\n",
    "############## GENERATE DATA #############\n",
    "\n",
    "n_trials_train = 100000\n",
    "n_trials_test  = 10000\n",
    "stim_duration  = 0.5\n",
    "wait_time      = 0\n",
    "sampling_freq  = 1000\n",
    "sum_rate       = 200\n",
    "n_samples = (stim_duration+wait_time)*sampling_freq + 1\n",
    "regenerate_datasets = True\n",
    "\n",
    "def generate_dataset():\n",
    "\n",
    "    t_start = time.time()\n",
    "    click_bias_train  = np.random.uniform( 0, 1, (n_trials_train))\n",
    "    click_bias_test   = np.random.uniform( 0, 1, (n_trials_test) )\n",
    "    (X_train,Y_train) = generate_poisson_dataset.generate_click_dataset( \n",
    "        n_trials_train,  click_bias_train, n_samples, \n",
    "        sum_rate=sum_rate, stim_duration=stim_duration, \n",
    "        wait_time=wait_time, sampling_freq=sampling_freq, \n",
    "        centered=False,choice_values=choice_values)\n",
    "    (X_test,Y_test)   = generate_poisson_dataset.generate_click_dataset( \n",
    "        n_trials_test,  click_bias_test, n_samples, \n",
    "        sum_rate=sum_rate, stim_duration=stim_duration,  \n",
    "        wait_time=wait_time,  sampling_freq=sampling_freq, \n",
    "        centered=False,choice_values=choice_values)\n",
    "    \n",
    "    print('Datasets generated in %.1f seconds' %( time.time() - t_start))\n",
    "\n",
    "    if params.do_center_X:\n",
    "        print('Centering datasets...')\n",
    "        for row in range(X_train.shape[0]):\n",
    "            X_train[row,:,:] = X_train[row,:,:] - np.mean( X_train[row,:,:] )\n",
    "        for row in range(X_test.shape[0]):\n",
    "            X_test[row,:,:] = X_test[row,:,:] - np.mean( X_test[row,:,:] )\n",
    "    else:\n",
    "        print('Not centering datasets.')\n",
    "    return (X_train,Y_train,X_test,Y_test)\n",
    "        \n",
    "print('\\nDone with cell at',datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n",
      "Datasets generated in 15.3 seconds\n",
      "Centering datasets...\n",
      "Compiling with parameters as follows:\n",
      "-------------------  -----------------\n",
      "dense_input_dim      256\n",
      "dense_output_dim     1\n",
      "do_center_X          1\n",
      "dropout              0.75\n",
      "final_layer_type     'dense_hingeloss'\n",
      "inner_activation     'tanh'\n",
      "optimizer            'RMSprop'\n",
      "regenerate_datasets  0\n",
      "rmsprop_clipvalue    None\n",
      "rmsprop_lr           0.001\n",
      "rnn_1_hidden_dim     256\n",
      "rnn_1_input_dim      2\n",
      "rnn_init_scale       1\n",
      "rnn_inner_init_type  'identity'\n",
      "rnn_inner_scale      1\n",
      "rnn_output_scale     1\n",
      "-------------------  -----------------\n",
      "Building  model...\n",
      "Compiling\n",
      "...done compiling in 2.6 seconds 2016-01-22 20:40:30.933021\n",
      "Starting training at  2016-01-22 20:40:30.988577\n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "100000/100000 [==============================] - 146s - loss: 0.8420 - val_loss: 0.7772\n",
      "Epoch 00000: val_loss improved from inf to 0.77720, saving model to /home/avaughan/Dropbox/rnn/checkpoint_2016-01-22_20-40-30.hdf5\n",
      "Epoch 2/300\n",
      "100000/100000 [==============================] - 146s - loss: 0.6651 - val_loss: 0.5578\n",
      "Epoch 00001: val_loss improved from 0.77720 to 0.55782, saving model to /home/avaughan/Dropbox/rnn/checkpoint_2016-01-22_20-40-30.hdf5\n",
      "Epoch 3/300\n",
      "100000/100000 [==============================] - 148s - loss: 0.4162 - val_loss: 0.3312\n",
      "Epoch 00002: val_loss improved from 0.55782 to 0.33118, saving model to /home/avaughan/Dropbox/rnn/checkpoint_2016-01-22_20-40-30.hdf5\n",
      "Epoch 4/300\n",
      "100000/100000 [==============================] - 148s - loss: 0.4005 - val_loss: 0.4464\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/300\n",
      "100000/100000 [==============================] - 147s - loss: 0.5753 - val_loss: 0.7093\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 6/300\n",
      "100000/100000 [==============================] - 147s - loss: 0.6935 - val_loss: 0.7059\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/300\n",
      "100000/100000 [==============================] - 148s - loss: 0.6927 - val_loss: 0.7065\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 8/300\n",
      "100000/100000 [==============================] - 147s - loss: 0.6928 - val_loss: 0.7037\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/300\n",
      "100000/100000 [==============================] - 148s - loss: 0.6904 - val_loss: 0.7004\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/300\n",
      "100000/100000 [==============================] - 149s - loss: 0.6891 - val_loss: 0.6993\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/300\n",
      "100000/100000 [==============================] - 150s - loss: 0.6868 - val_loss: 0.6975\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 12/300\n",
      "100000/100000 [==============================] - 146s - loss: 0.6855 - val_loss: 0.6968\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 13/300\n",
      "100000/100000 [==============================] - 147s - loss: 0.6867 - val_loss: 0.6992\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 14/300\n",
      "100000/100000 [==============================] - 147s - loss: 0.6893 - val_loss: 0.7005\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Done with cell at 2016-01-22 21:15:00.562484\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "######### GENERATE DATSET AND RUN MODEL ##############\n",
    "######################################################\n",
    "\n",
    "####### Compile the actual model #########\n",
    "params = SimpleNameSpace()\n",
    "\n",
    "params.rnn_1_input_dim               = 2\n",
    "params.rnn_1_hidden_dim              = 256 # = number of hidden units.\n",
    "\n",
    "params.rnn_init_scale                = 1 # Normal, scaled by sigma (std).  ~0.001ish might be reasonable.\n",
    "params.rnn_inner_init_type           = 'identity' \n",
    "#params.rnn_inner_init_type           = 'normal_radius' \n",
    "#params.rnn_inner_init_type           = 'orthogonal'\n",
    "params.rnn_inner_scale               = 1\n",
    "params.inner_activation              = 'tanh' #'tanh','sigmoid', 'relu','softplus'\n",
    "params.rnn_output_scale              = 1 # 1e-6\n",
    "\n",
    "params.final_layer_type              = 'dense_hingeloss'  # ok with tanh, bad with relu (2-3 rounds) or softplus? \n",
    "#params.final_layer_type              = 'sigmoid_hingeloss' # Works with softplus, totally fails with relu\n",
    "#params.final_layer_type              = 'softmax_binary_crossentropy' #-> seems stuck\n",
    "params.dense_input_dim               = params.rnn_1_hidden_dim\n",
    "params.dense_output_dim              = 1\n",
    "\n",
    "params.dropout                       = 0.75 \n",
    "\n",
    "# params.optimizer                     = 'SGD'\n",
    "# params.sgd_lr                        = 1e-5 # 1e-4 by default.\n",
    "# params.sgd_decay                     = 1e-5\n",
    "# params.sgd_momentum                  = 0.995\n",
    "# params.sgd_nesterov                  = True\n",
    "# params.sgd_clipvalue                   = 10\n",
    "\n",
    "params.optimizer                    = 'RMSprop'\n",
    "params.rmsprop_lr                   = 0.001 #0.001 # 0.001 by default.\n",
    "params.rmsprop_clipvalue            = None\n",
    "\n",
    "params.do_center_X                   = 1\n",
    "params.regenerate_datasets           = 0\n",
    "\n",
    "############## Sanity check on inputs #############\n",
    "\n",
    "####### Generate dataset #######\n",
    "if params.final_layer_type   == 'dense_hingeloss':             choice_values = (-1, 1)\n",
    "elif params.final_layer_type == 'softmax_binary_crossentropy': choice_values = ( 0, 1)\n",
    "elif params.final_layer_type == 'sigmoid_hingeloss':           choice_values = ( 0, 1)\n",
    "else: raise Exception('params.final_layer_type not found :: ',params.final_layer_type)\n",
    "if 'X_train' not in locals() or X_train is None or regenerate_datasets is True or X_train.shape[1] != n_samples:\n",
    "        print('Generating dataset...')\n",
    "        (X_train,Y_train,X_test,Y_test) =  generate_dataset()\n",
    "        Y_test_vals = np.squeeze(Y_test[:,-1,:])\n",
    "        Y_train_vals = np.squeeze(Y_train[:,-1,:])\n",
    "else:\n",
    "    print('Reusing previous datasets.')\n",
    "#(nb_samples, timesteps, input_dim)\n",
    "# print('X_train.shape :: ',X_train.shape)\n",
    "# print('Y_train full :: ',Y_train.shape)\n",
    "# print('Y_train last :: ',np.squeeze(Y_train[:,-1,:]))\n",
    "# print('Targeting choice values ',choice_values,'with final layer :: ',params.final_layer_type)\n",
    "\n",
    "if not set(Y_test_vals) == set(choice_values):\n",
    "    print('WARNING :: choice_values does not match targets in Y_train')\n",
    "\n",
    "############### COMPILE MODEL ###########\n",
    "\n",
    "model = define_and_compile_model(params)\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(filename='model.png') \n",
    "\n",
    "# ############# Begin training #############\n",
    "print('Starting training at ',datetime.now())\n",
    "checkpoint_time = [t[:19] for t in (str(datetime.now()),)]\n",
    "checkpoint_path = \"/home/avaughan/Dropbox/rnn/checkpoint_\" + checkpoint_time[0].replace(\" \",\"_\").replace(\":\",\"-\") + \".hdf5\"\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "earlystopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1) # Should take a mode?\n",
    "nanchecker   = callbacks.NaNChecker(monitor='loss')\n",
    "\n",
    "model.fit(  X_train, Y_train_vals, \n",
    "          nb_epoch=300, batch_size=128, \n",
    "          shuffle = 1, verbose=1, validation_data=(X_test, Y_test_vals), \n",
    "          callbacks=[checkpointer,earlystopper,nanchecker] )\n",
    "\n",
    "print('\\nDone with cell at',datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-372a36261033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_trials_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclick_bias_validate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_trials_validate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m (X_validate,Y_validate)   = generate_poisson_dataset.generate_click_dataset( \n\u001b[1;32m      7\u001b[0m     \u001b[0mn_trials_validate\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mclick_bias_validate\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "##### ANALYZE RESPONSES #####\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "n_trials_validate = 1000\n",
    "click_bias_validate  = np.random.uniform( 0, 1, (n_trials_validate) )\n",
    "(X_validate,Y_validate)   = generate_poisson_dataset.generate_click_dataset( \n",
    "    n_trials_validate,  click_bias_validate,  n_samples,\n",
    "    sum_rate=sum_rate, stim_duration=stim_duration,  wait_time=wait_time,  \n",
    "    sampling_freq=sampling_freq, centered=False,choice_values=choice_values)\n",
    "Y_validate_vals = np.squeeze(Y_validate[:,-1,:])\n",
    "left_trials  = Y_validate[:,-1,0] == choice_values[0]\n",
    "right_trials = Y_validate[:,-1,0] == choice_values[1]\n",
    "print('\\n Parameters of validation set :: ')\n",
    "print ('mean( left clicks  | right trial ) :: %.0f ' % ( np.mean(np.sum(X_validate[ right_trials , :(sampling_freq*stim_duration),0], axis=1)) ))\n",
    "print ('mean( right clicks | right trial ) :: %.0f ' % ( np.mean(np.sum(X_validate[ right_trials , :(sampling_freq*stim_duration),1], axis=1)) ))\n",
    "print ('')\n",
    "print ('N left choices  :: %.0f ' % (sum(Y_train[:,-1,0] == -1)))\n",
    "print ('mean( left clicks  | left trial  ) :: %.0f ' % ( np.mean(np.sum(X_validate[ left_trials , :(sampling_freq*stim_duration),0], axis=1)) ))\n",
    "print ('mean( right cliks  | left trial  ) :: %.0f ' % ( np.mean(np.sum(X_validate[ left_trials , :(sampling_freq*stim_duration),1], axis=1)) ))\n",
    "print ('')\n",
    "print ('    min :: %.2f' % (min(X_validate.ravel())))\n",
    "print ('    max :: %.2f' % (max(X_validate.ravel())))\n",
    "print ('    mean :: %.2f' % (np.mean(X_validate.ravel())))\n",
    "print ('\\n')\n",
    "\n",
    "n = 100\n",
    "\n",
    "#model.fit(X_train, Y_train[:,-1,0], nb_epoch=20, batch_size=5) # Last sample\n",
    "#model.fit(X_train, Y_train, nb_epoch=10, batch_size=100, validation_split=0.1)\n",
    "objective_score = model.evaluate(X_validate, Y_validate_vals, batch_size=100)\n",
    "print('Objective score (',params.final_layer_type,') :: ',objective_score)\n",
    "\n",
    "validation_prediction = model.predict(X_validate)\n",
    "\n",
    "if params.final_layer_type   == 'dense_hingeloss':             round_fn = lambda val: (val > 0) * 2 - 1\n",
    "elif params.final_layer_type == 'softmax_binary_crossentropy': round_fn = lambda val: round(val)\n",
    "elif params.final_layer_type == 'sigmoid_hingeloss':           round_fn = lambda val: round(val)\n",
    "else: raise Exception('params.final_layer_type not found :: ', params.final_layer_type)\n",
    "\n",
    "validation_prediction_round = round_fn(validation_prediction)\n",
    "\n",
    "# print a.shape\n",
    "# for i in range(a.shape[0]):\n",
    "#     plt.plot(a[i,])\n",
    "#     plt.ylim( (0,1) )\n",
    "print ('Shape of y_test :: ',         Y_validate[:10,-1,:].shape)\n",
    "print ('Uniques in y_test :: ',       np.unique(Y_validate[:10,-1,:]))\n",
    "print ('Shape of test_prediction :: ',validation_prediction.shape)\n",
    "#print ('Uniques in test_prediction       :: ', np.unique(validation_prediction))\n",
    "#print ('Uniques in test_prediction_round :: ', np.unique(validation_prediction_round))\n",
    "uniques = np.unique(validation_prediction_round)\n",
    "print('Sum of test_prediction_round = uniques[0] :: ',uniques[0],' :: ',sum(validation_prediction_round == uniques[0]))\n",
    "print('Sum of test_prediction_round = uniques[1] :: ',uniques[1],' :: ',sum(validation_prediction_round == uniques[1]))\n",
    "\n",
    "\n",
    "# Pull responses and run t-test\n",
    "def pred_score(target):\n",
    "    return validation_prediction_round[np.where(Y_validate[:n,-1,0] == target)]\n",
    "def p_stars(p_value):\n",
    "    if p < 0.01: return '*'\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "# PLOT RESPONSES\n",
    "plt.figure()\n",
    "plt.plot( Y_validate[:n,-1,0] + 0.1 * (np.random.uniform(size=Y_validate[:n,-1,0].shape)-0.5), validation_prediction[:n,0],'.')\n",
    "plt.violinplot( (pred_score(choice_values[0]) , pred_score(choice_values[1])), positions=choice_values,showmeans=True)\n",
    "plt.xlim(np.asarray(choice_values) + [-0.5,0.5])\n",
    "\n",
    "def sigmoid(x, x0, k):\n",
    "     y = 1 / (1 + np.exp(-k*(x-x0)))\n",
    "     return y\n",
    "popt, pcov = curve_fit(sigmoid, click_bias_validate.ravel(), validation_prediction.ravel())\n",
    "sigmoid_bias       = np.linspace(0,1, 50)\n",
    "sigmoid_prediction = sigmoid(sigmoid_bias, *popt)\n",
    "plt.figure()\n",
    "plt.plot(click_bias_validate.ravel(),validation_prediction + 0.1 * np.random.uniform(size=validation_prediction.shape),'.')\n",
    "plt.plot(sigmoid_bias,sigmoid_prediction)\n",
    "plt.xlabel('Nominal click rate')\n",
    "plt.ylabel('Prediction (raw)')\n",
    "\n",
    "# MEAN RESPONSE VALUES\n",
    "print('Choice ',choice_values[0],' :: ',np.mean(pred_score(choice_values[0])),'+/-',  np.var(pred_score(choice_values[0])))\n",
    "print('Choice ',choice_values[1],' :: ',np.mean(pred_score(choice_values[1])),'+/-', np.var(pred_score(choice_values[1])))\n",
    "\n",
    "# STATS\n",
    "[_,p] = sp.stats.ttest_ind(pred_score(choice_values[0]),pred_score(choice_values[1]))\n",
    "print('p(distributions of response variables are identical) == ',p[0],' ',p_stars(p))\n",
    "\n",
    "# Crosstab\n",
    "#pandas.crosstab( np.ravel(validation_prediction_round), np.ravel(Y_validate[:n,-1,0]) ,rownames=['Prediction'],colnames=['Actual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ SANITY CHECK / DEBUG DATSET  #####################\n",
    "\n",
    "# Generate a dataset of poisson spikes.  Basically just calls generate_click_dataset.\n",
    "# X_train is set up as n_samples, n_dimensions=2, n_timepoints\n",
    "# Y_Train is set up as n_samples, n_dimensions=1, n_timepoints\n",
    "# Cost function will have to match Y_train (ie., only match the end)\n",
    "\n",
    "n_trials_train = 20000\n",
    "n_trials_test  = 2000\n",
    "stim_duration  = 1\n",
    "wait_time      = 0\n",
    "sampling_freq  = 1000\n",
    "sum_rate       = 100\n",
    "n_samples = (stim_duration+wait_time)*sampling_freq + 1\n",
    "choice_values = (-1,1) # \n",
    "\n",
    "#click_bias_train = np.random.normal(0.5,0.2,(n_trials_train))\n",
    "#click_bias_test = np.random.normal(0.5,0.2,(n_trials_test))\n",
    "#click_bias_train[click_bias_train <= 0.1] = 0.1\n",
    "#click_bias_test[click_bias_test <= 0.1]   = 0.1\n",
    "\n",
    "click_bias_train = np.random.uniform( 0, 1, (n_trials_train))\n",
    "click_bias_test  = np.random.uniform( 0, 1, (n_trials_test) )\n",
    "\n",
    "t_start = time.time()\n",
    "(X_train,Y_train) = generate_poisson_dataset.generate_click_dataset( n_trials_train, \n",
    "                                           click_bias_train, \n",
    "                                           n_samples,\n",
    "                                           sum_rate=sum_rate, \n",
    "                                           stim_duration=stim_duration, \n",
    "                                           wait_time=wait_time, \n",
    "                                           sampling_freq=sampling_freq, \n",
    "                                           centered=False)\n",
    "(X_test,Y_test)   = generate_poisson_dataset.generate_click_dataset( n_trials_test,  \n",
    "                                           click_bias_test,\n",
    "                                           n_samples,\n",
    "                                           sum_rate=sum_rate, \n",
    "                                           stim_duration=stim_duration, \n",
    "                                           wait_time=wait_time, \n",
    "                                           sampling_freq=sampling_freq, \n",
    "                                           centered=False)\n",
    "\n",
    "# Center\n",
    "#X_train = X_train - np.mean(X_train.ravel())\n",
    "#X_test  = X_test - np.mean(X_test.ravel())\n",
    "\n",
    "print('Datasets generated in %.1f seconds' %( time.time() - t_start))\n",
    "print('X_train.shape :: %s' %(str(X_train.shape)))\n",
    "print('Y_train.shape :: %s' %(str(Y_train.shape)))\n",
    "print('X_test.shape  :: %s' %(str(X_test.shape)))\n",
    "print('Y_test.shape  :: %s' %(str(Y_test.shape)))\n",
    "print(''     )\n",
    "print('N right choices :: %.0f ' % (sum(Y_train[:,-1,0] == 1)))\n",
    "\n",
    "left_trials  = Y_test[:,-1,0] == choice_values[0]\n",
    "right_trials = Y_test[:,-1,0] == choice_values[1]\n",
    "print ('mean( left clicks  | right trial ) :: %.0f ' % ( np.mean(np.sum(X_test[ right_trials , :(sampling_freq*stim_duration),0], axis=1)) ))\n",
    "print ('mean( right clicks | right trial ) :: %.0f ' % ( np.mean(np.sum(X_test[ right_trials , :(sampling_freq*stim_duration),1], axis=1)) ))\n",
    "print ('')\n",
    "print ('N left choices  :: %.0f ' % (sum(Y_train[:,-1,0] == -1)))\n",
    "print ('mean( left clicks  | left trial  ) :: %.0f ' % ( np.mean(np.sum(X_test[ left_trials , :(sampling_freq*stim_duration),0], axis=1)) ))\n",
    "print ('mean( right cliks  | left trial  ) :: %.0f ' % ( np.mean(np.sum(X_test[ left_trials , :(sampling_freq*stim_duration),1], axis=1)) ))\n",
    "print ('')\n",
    "print ('    min :: %.2f' % (min(X_test.ravel())))\n",
    "print ('    max :: %.2f' % (max(X_test.ravel())))\n",
    "print ('    mean :: %.2f' % (np.mean(X_test.ravel())))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(X_train[:,:,0],aspect='auto')\n",
    "plt.ylabel('Left Trials (=0)')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(X_train[:,:,1],aspect='auto')\n",
    "plt.ylabel('Right Trials (=1)')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(X_train[0,:100,0],'.')\n",
    "plt.ylim((-1.1,1.1))\n",
    "plt.title('Trial 1 : type %.0f' %(Y_train[0,-1,0]))\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(X_train[0,:100,1],'.')\n",
    "plt.ylim((-1.1,1.1))\n",
    "\n",
    "print('\\nDone with cell at',datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.uniform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
